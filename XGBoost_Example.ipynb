{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Kinecal Data ELEC872 Project\n",
    "### Cameron Bishop | November 16th 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from 'c:\\\\Users\\\\camb7\\\\Code Repositories\\\\KINECAL Balance Assessment\\\\KINECAL-Balance-Assessment\\\\models.py'>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize, LabelEncoder #Standardization and conversion of categorical labels to binary representation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report, plot_roc_curve\n",
    "\n",
    "import LoadKinecalFunctions\n",
    "from LoadKinecalFunctions import *\n",
    "importlib.reload(LoadKinecalFunctions)\n",
    "from const import User, Exercise\n",
    "import models\n",
    "from models import *\n",
    "importlib.reload(models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = User.CD\n",
    "excerciseList = [Exercise.QSECFS, Exercise.QSEOFS, Exercise.STS, Exercise.TS]\n",
    "\n",
    "QSClosedDf = readKinecalFiles(excerciseList[0], user) \n",
    "QSOpenDf = readKinecalFiles(excerciseList[1], user) \n",
    "SemiTandemDf = readKinecalFiles(excerciseList[2], user) \n",
    "TandemDf = readKinecalFiles(excerciseList[3], user) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate X and Y Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   part_id group sex  height  weight   BMI recorded_in_the_lab  \\\n",
      "0       10    HA   m    1.85   77.00 22.50                   1   \n",
      "1      100    NF   f    1.55   82.50 34.30                   1   \n",
      "2       11    HA   f    1.57   51.50 20.90                   1   \n",
      "3       12    HA   m    1.64   68.50 25.50                   1   \n",
      "4       13    HA   m    1.78   85.00 26.80                   1   \n",
      "..     ...   ...  ..     ...     ...   ...                 ...   \n",
      "85      84   FHm   f    1.56   60.00 24.70                   0   \n",
      "86      87   FHs   f    1.60   77.10 30.10                   0   \n",
      "87       9    HA   m    1.69   75.00 26.30                   1   \n",
      "88      92    NF   m    1.83   97.00 29.00                   0   \n",
      "89      96   FHs   m    1.78   81.20 25.60                   0   \n",
      "\n",
      "   clinically_at_risk  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                   0  \n",
      "3                   0  \n",
      "4                   0  \n",
      "..                ...  \n",
      "85                  0  \n",
      "86                  1  \n",
      "87                  0  \n",
      "88                  0  \n",
      "89                  0  \n",
      "\n",
      "[90 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "yDf = QSClosedDf.iloc[:, 0:9]\n",
    "yDf = yDf.drop(columns='movement')\n",
    "\n",
    "QSClosedXDf = QSClosedDf.iloc[:, 9:-1]\n",
    "QSOpenXDf = QSOpenDf.iloc[:, 9:-1]\n",
    "SemiTandemXDf = SemiTandemDf.iloc[:, 9:-1]\n",
    "TandemXDf = TandemDf.iloc[:, 9:-1]\n",
    "\n",
    "print(yDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Missing Data With Mean of That Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 14 15]\n",
      "[ 2 17 29 34 71]\n",
      "[ 2 14 17 31 33 60 66]\n",
      "[13 14 17 29 31 33 36 37 55 60 66 74 77]\n"
     ]
    }
   ],
   "source": [
    "QSClosedXDf = replaceMissingValues(QSClosedXDf, yDf)\n",
    "QSOpenXDf = replaceMissingValues(QSOpenXDf, yDf)\n",
    "SemiTandemXDf = replaceMissingValues(SemiTandemXDf, yDf)\n",
    "TandemXDf = replaceMissingValues(TandemXDf, yDf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Sample Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.asarray(QS_Closed_x_df.iloc[0,:].values))\n",
    "QSClosedXNormDf = datasetNormalization(QSClosedXDf)\n",
    "QSOpenXNormDf = datasetNormalization(QSOpenXDf)\n",
    "SemiTandemXNormDf = datasetNormalization(SemiTandemXDf)\n",
    "TandemXNormDf = datasetNormalization(TandemXDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Excercises Into One Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 16) (90, 16) (90, 16) (90, 16)\n",
      "(90, 64)\n"
     ]
    }
   ],
   "source": [
    "print(QSClosedXNormDf.shape, QSOpenXNormDf.shape, SemiTandemXNormDf.shape, TandemXNormDf.shape)\n",
    "combinedXNormDf = pd.concat([QSClosedXNormDf, QSOpenXNormDf, SemiTandemXNormDf, TandemXDf], axis=1)\n",
    "print(combinedXNormDf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate Into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   part_id group sex  height  weight   BMI recorded_in_the_lab  \\\n",
      "49      42   FHm   f    1.66   87.00 31.60                   0   \n",
      "62      51    NF   f    1.58   55.30 22.20                   0   \n",
      "73     701   FHs   f    1.65   85.00 31.20                   0   \n",
      "69      67    NF   f    1.68   63.50 22.50                   0   \n",
      "76     705   FHs   m    1.88   87.00 24.60                   0   \n",
      "..     ...   ...  ..     ...     ...   ...                 ...   \n",
      "20      26    HA   m    1.73   76.00 25.40                   1   \n",
      "60     504    HA   m    1.71   70.00 23.90                   1   \n",
      "71       7    HA   f    1.70   59.00 20.40                   1   \n",
      "14     201   FHm   f    1.52   54.00 23.40                   0   \n",
      "51      44   FHs   f    1.65   50.80 18.70                   0   \n",
      "\n",
      "   clinically_at_risk  \n",
      "49                  0  \n",
      "62                  0  \n",
      "73                  0  \n",
      "69                  0  \n",
      "76                  0  \n",
      "..                ...  \n",
      "20                  0  \n",
      "60                  0  \n",
      "71                  0  \n",
      "14                  1  \n",
      "51                  0  \n",
      "\n",
      "[72 rows x 8 columns]\n",
      "   part_id group sex  height  weight   BMI recorded_in_the_lab  \\\n",
      "40     400   FHs   f    1.73   66.70 22.30                   0   \n",
      "22     300    HA   f    1.57   49.80 20.20                   0   \n",
      "55      48    NF   m    1.70   73.40 25.40                   0   \n",
      "70      68    NF   f    1.68   69.00 24.40                   0   \n",
      "0       10    HA   m    1.85   77.00 22.50                   1   \n",
      "26     304   FHs   f    1.63   60.30 22.70                   0   \n",
      "39      40   FHs   m    1.90  102.50 28.40                   0   \n",
      "65      61    NF   m    1.73   65.30 21.80                   0   \n",
      "10      19    HA   m    1.78   79.00 24.90                   1   \n",
      "44     404   FHm   f    1.57   63.00 25.60                   0   \n",
      "81      73    NF   m    1.78   69.80 22.00                   0   \n",
      "35     313    NF   f    1.73   75.00 25.10                   0   \n",
      "56       5    HA   m    1.75   70.00 22.90                   1   \n",
      "86      87   FHs   f    1.60   77.10 30.10                   0   \n",
      "12      20    HA   m    1.66   60.00 21.80                   1   \n",
      "4       13    HA   m    1.78   85.00 26.80                   1   \n",
      "18      24    HA   f    1.57   73.00 29.60                   0   \n",
      "28     306    HA   m    1.78   70.70 22.30                   0   \n",
      "\n",
      "   clinically_at_risk  \n",
      "40                  0  \n",
      "22                  0  \n",
      "55                  0  \n",
      "70                  0  \n",
      "0                   0  \n",
      "26                  0  \n",
      "39                  0  \n",
      "65                  0  \n",
      "10                  0  \n",
      "44                  0  \n",
      "81                  0  \n",
      "35                  0  \n",
      "56                  0  \n",
      "86                  1  \n",
      "12                  0  \n",
      "4                   0  \n",
      "18                  0  \n",
      "28                  0  \n",
      "[0 3 1 3 1 2 3 2 0 1 0 3 2 1 2 3 3 2 3 3 2 3 3 2 3 2 3 1 2 2 2 2 2 2 2 3 1\n",
      " 3 1 3 3 3 3 2 3 2 0 1 3 3 3 2 1 0 3 2 1 0 3 3 3 0 2 2 3 3 2 2 2 2 0 1]\n"
     ]
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(combinedXNormDf, yDf, test_size=0.2, random_state=42)\n",
    "\n",
    "print(yTrain)\n",
    "print(yTest)\n",
    "\n",
    "le = LabelEncoder()\n",
    "yTest = le.fit_transform(yTest.group.values)\n",
    "yTrain = le.fit_transform(yTrain.group.values)\n",
    "print(yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3456 candidates, totalling 17280 fits\n",
      "0.6514285714285715\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.5, 'max_depth': 2, 'min_child_weight': 4, 'missing': -999, 'n_estimators': 225, 'objective': 'multi:softmax', 'seed': 1337, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "tuning_params =[{\n",
    "    'learning_rate': [0.3, 0.4, 0.5],\n",
    "    'objective': ['multi:softmax'],\n",
    "    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_child_weight': [2, 3, 4],\n",
    "    'subsample': [0.6, 0.65, 0.7],\n",
    "    'colsample_bytree': [0.65 ,0.7, 0.75, 0.8],\n",
    "    'n_estimators': [200, 225, 250, 275],\n",
    "    \"missing\": [-999],\n",
    "    \"seed\": [1337]\n",
    "}]\n",
    "xTest = np.asarray(xTest)\n",
    "xTrain = np.asarray(xTrain)\n",
    "grid = GridSearchCV(xgboost.XGBClassifier(), tuning_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=2, refit=True)\n",
    "grid.fit(xTrain, yTrain)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "# xgb_model = tune_xgboost(xTrain, yTrain, tuning_params, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 34.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
      "Model                                                                          \n",
      "BernoulliNB                        0.39               0.53    None      0.38   \n",
      "LinearSVC                          0.44               0.37    None      0.45   \n",
      "RidgeClassifier                    0.44               0.37    None      0.45   \n",
      "RandomForestClassifier             0.44               0.37    None      0.44   \n",
      "BaggingClassifier                  0.39               0.33    None      0.38   \n",
      "ExtraTreesClassifier               0.44               0.32    None      0.43   \n",
      "LabelPropagation                   0.39               0.32    None      0.43   \n",
      "LabelSpreading                     0.39               0.32    None      0.43   \n",
      "LogisticRegression                 0.39               0.32    None      0.39   \n",
      "PassiveAggressiveClassifier        0.39               0.31    None      0.38   \n",
      "RidgeClassifierCV                  0.39               0.31    None      0.39   \n",
      "SVC                                0.39               0.29    None      0.32   \n",
      "DecisionTreeClassifier             0.33               0.29    None      0.34   \n",
      "SGDClassifier                      0.33               0.29    None      0.33   \n",
      "Perceptron                         0.33               0.27    None      0.37   \n",
      "AdaBoostClassifier                 0.33               0.26    None      0.32   \n",
      "KNeighborsClassifier               0.33               0.26    None      0.34   \n",
      "XGBClassifier                      0.33               0.26    None      0.34   \n",
      "DummyClassifier                    0.28               0.25    None      0.12   \n",
      "CalibratedClassifierCV             0.33               0.24    None      0.28   \n",
      "LGBMClassifier                     0.33               0.24    None      0.34   \n",
      "GaussianNB                         0.22               0.19    None      0.22   \n",
      "NearestCentroid                    0.22               0.19    None      0.25   \n",
      "QuadraticDiscriminantAnalysis      0.22               0.17    None      0.22   \n",
      "ExtraTreeClassifier                0.22               0.17    None      0.25   \n",
      "LinearDiscriminantAnalysis         0.17               0.14    None      0.17   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "BernoulliNB                          0.01  \n",
      "LinearSVC                            0.03  \n",
      "RidgeClassifier                      0.01  \n",
      "RandomForestClassifier               0.14  \n",
      "BaggingClassifier                    0.03  \n",
      "ExtraTreesClassifier                 0.12  \n",
      "LabelPropagation                     0.01  \n",
      "LabelSpreading                       0.01  \n",
      "LogisticRegression                   0.02  \n",
      "PassiveAggressiveClassifier          0.01  \n",
      "RidgeClassifierCV                    0.01  \n",
      "SVC                                  0.01  \n",
      "DecisionTreeClassifier               0.01  \n",
      "SGDClassifier                        0.01  \n",
      "Perceptron                           0.01  \n",
      "AdaBoostClassifier                   0.07  \n",
      "KNeighborsClassifier                 0.01  \n",
      "XGBClassifier                        0.09  \n",
      "DummyClassifier                      0.01  \n",
      "CalibratedClassifierCV               0.10  \n",
      "LGBMClassifier                       0.06  \n",
      "GaussianNB                           0.01  \n",
      "NearestCentroid                      0.01  \n",
      "QuadraticDiscriminantAnalysis        0.01  \n",
      "ExtraTreeClassifier                  0.01  \n",
      "LinearDiscriminantAnalysis           0.01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lazyclass = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, preds = lazyclass.fit(xTrain, xTest, yTrain, yTest)\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Optimal XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimalParameters = [{'kernel': ['rbf'], 'gamma':['scale'],'C':[250]}]\n",
    "#svc = train_svm(xTrain,yTrain.group.values.shape,optimalParameters,modelDir,modelName)\n",
    "\n",
    "clf = svm.SVC(kernel = 'rbf' , C = 250, gamma='scale', probability=True)\n",
    "clf.fit(xTrain, yTrain.group.values)\n",
    "\n",
    "y_pred = clf.predict(xTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Confusion Matrix and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['HA','NF','FHs','FHm']\n",
    "cm_python = confusion_matrix(y_true=yTest.group.values, y_pred=y_pred, labels=classes)\n",
    "\n",
    "plt.subplots(figsize=(6,4))\n",
    "\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                cm_python.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     cm_python.flatten()/np.sum(cm_python)]\n",
    "labels = [f'{v1}\\n{v2}' for v1, v2 in\n",
    "          zip(group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(4,4)\n",
    "df_cm_python = pd.DataFrame(cm_python)\n",
    "\n",
    "sns.heatmap(df_cm_python, annot=labels, fmt='',annot_kws={\"size\": 16}, xticklabels=classes, yticklabels=classes) # font size\n",
    "plt.title('Test Model: Standard processing w/normalization')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(classification_report(y_true=yTest.group.values, y_pred=y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap Labels to Be Binary Single Fallers Included as Fallers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrainBinary = binaryLabelRemapping(yTrain, True)\n",
    "yTestBinary = binaryLabelRemapping(yTest, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-4, 1e-5, 'scale', 'auto'], 'C': [100, 250, 500, 750, 1000]},\n",
    "                        {'kernel': ['sigmoid'], 'gamma': [1e-4, 1e-5, 'scale', 'auto'], 'C': [100, 250, 500, 750, 1000]}]\n",
    "modelDir = './models/'\n",
    "modelName = 'tunedSVM5Fold_binarylabelsTrue'\n",
    "svmModel = tune_svm(xTrain,yTrainBinary.label_binary.values, tuned_parameters, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost with Optimal Parameters For Binary Labels Single Fallers are Fallers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel = 'rbf' , C = 250, gamma='scale', probability=True)\n",
    "clf.fit(xTrain, yTrainBinary.label_binary.values)\n",
    "\n",
    "y_pred = clf.predict(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Confusion Matrix For Binary Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Faller', 'Non-Faller']\n",
    "cm_python = confusion_matrix(y_true=yTestBinary.label_binary.values, y_pred=y_pred, labels=classes)\n",
    "\n",
    "plt.subplots(figsize=(6,4))\n",
    "\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                cm_python.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     cm_python.flatten()/np.sum(cm_python)]\n",
    "labels = [f'{v1}\\n{v2}' for v1, v2 in\n",
    "          zip(group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "df_cm_python = pd.DataFrame(cm_python)\n",
    "\n",
    "sns.heatmap(df_cm_python, annot=labels, fmt='',annot_kws={\"size\": 16}, xticklabels=classes, yticklabels=classes) # font size\n",
    "plt.title('Test Model: Standard processing w/normalization')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(classification_report(y_true=yTestBinary.label_binary.values, y_pred=y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Missclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclassifications = yTestBinary.loc[yTestBinary.label_binary!=y_pred]\n",
    "print(missclassifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap Labels to Be Binary Single Fallers Included as Non-Fallers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrainBinary = binaryLabelRemapping(yTrain, False)\n",
    "yTestBinary = binaryLabelRemapping(yTest, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-4, 1e-5, 'scale', 'auto'], 'C': [100, 250, 500, 750, 1000]},\n",
    "                        {'kernel': ['sigmoid'], 'gamma': [1e-4, 1e-5, 'scale', 'auto'], 'C': [100, 250, 500, 750, 1000]}]\n",
    "modelDir = './models/'\n",
    "modelName = 'tunedSVM5Fold_binarylabelsFalse'\n",
    "svmModel = tune_svm(xTrain,yTrainBinary.label_binary.values, tuned_parameters, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost with Optimal Parameters Single Fallers Considered Non-Fallers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel = 'sigmoid' , C = 750, gamma=0.0001, probability=True)\n",
    "clf.fit(xTrain, yTrainBinary.label_binary.values)\n",
    "\n",
    "y_pred = clf.predict(xTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Confusion Matrix For Binary Labels FHs -> Non-Fallers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Faller', 'Non-Faller']\n",
    "cm_python = confusion_matrix(y_true=yTestBinary.label_binary.values, y_pred=y_pred, labels=classes)\n",
    "\n",
    "plt.subplots(figsize=(6,4))\n",
    "\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                cm_python.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     cm_python.flatten()/np.sum(cm_python)]\n",
    "labels = [f'{v1}\\n{v2}' for v1, v2 in\n",
    "          zip(group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "df_cm_python = pd.DataFrame(cm_python)\n",
    "\n",
    "sns.heatmap(df_cm_python, annot=labels, fmt='',annot_kws={\"size\": 16}, xticklabels=classes, yticklabels=classes) # font size\n",
    "plt.title('Test Model: Standard processing w/normalization')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(classification_report(y_true=yTestBinary.label_binary.values, y_pred=y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Missclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclassifications = yTestBinary.loc[yTestBinary.label_binary!=y_pred]\n",
    "print(missclassifications)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('kba')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d349dd511cd4e0f99132a14cf8fa9ad6fc13ca582fbb8d1253f3c76c77827a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
